# ğŸ“š Manual de Uso

GuÃ­a completa para usar todas las funcionalidades del sistema Video Whisper-Transcriptor.

## ğŸ¯ Flujo de Trabajo Completo

1. **Preparar vÃ­deos** â†’ Copiar a carpeta `videos/`
2. **Transcribir** â†’ Ejecutar `transcribir.py`
3. **Generar documentaciÃ³n** â†’ Ejecutar `generar_docs.py`
4. **Revisar resultados** â†’ Abrir archivos HTML generados

## ğŸ¬ TranscripciÃ³n de VÃ­deos

### Preparar VÃ­deos

```bash
# Estructura recomendada de nombres
videos/
â”œâ”€â”€ KK-F1-v1-Introduccion_a_la_IA.mp4
â”œâ”€â”€ KK-F1-v2-Chatbots_vs_IA.mp4
â”œâ”€â”€ KK-F2-v1-Navegacion_sistema.mp4
â””â”€â”€ KK-F2-v2-Gestion_agenda.mp4
```

**Nomenclatura importante:**
- `KK` = Prefijo del proyecto (Klinikare)
- `F1, F2, F3` = NÃºmero de fase
- `v1, v2, v3` = NÃºmero de vÃ­deo dentro de la fase

### Ejecutar TranscripciÃ³n

```bash
# Activar entorno virtual
source .venv/bin/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# Transcribir todos los vÃ­deos
python transcribir.py
```

### Opciones de TranscripciÃ³n

**MenÃº interactivo:**
```
ğŸ¬ TRANSCRIPTOR DE VÃDEOS KLINIKARE / CLINIQQUER ğŸ¬

ğŸ“‚ VÃ­deos encontrados: 4 archivos
ğŸ’¾ Total: 156.3 MB

ğŸ® OPCIONES:
   1. ğŸš€ Procesar TODOS los vÃ­deos
   2. ğŸ“ Seleccionar vÃ­deos especÃ­ficos
   3. ğŸ”„ Solo vÃ­deos nuevos
   0. ğŸšª Salir

ğŸ‘† Elige una opciÃ³n:
```

**Configuraciones disponibles:**
- **Modelo Whisper**: large-v3 (recomendado), medium, small
- **Idioma**: Auto-detecciÃ³n o espaÃ±ol especÃ­fico
- **Calidad**: float16 (rÃ¡pido) o float32 (mÃ¡xima calidad)

### Resultados de TranscripciÃ³n

Los archivos se guardan en `procesados/`:

```
procesados/
â”œâ”€â”€ transcripciones_20251113_192357.txt    # Transcripciones consolidadas
â”œâ”€â”€ registro_transcripciones.txt           # Log detallado
â”œâ”€â”€ estadisticas_transcripcion.json        # MÃ©tricas de rendimiento
â””â”€â”€ transcripciones_individuales/          # Archivos por vÃ­deo
    â”œâ”€â”€ KK-F1-v1-Introduccion_a_la_IA.txt
    â””â”€â”€ KK-F1-v2-Chatbots_vs_IA.txt
```

## ğŸ¤– GeneraciÃ³n de DocumentaciÃ³n

### MenÃº Principal

```bash
python generar_docs.py
```

**Opciones disponibles:**
```
ğŸ® MENÃš DE OPCIONES:

   1. ğŸŒ Generar con OpenAI GPT-4o (nube)
   2. ğŸ  Generar con Ollama GPT-OSS (local)
   3. ğŸ§  Generar con Ollama DeepSeek-R1 (local)
   4. ğŸ”„ Generar con TODOS (OpenAI + GPT-OSS + DeepSeek)
   5. ğŸ“‹ Mostrar transcripciones disponibles
   6. ğŸ”§ Verificar configuraciÃ³n
   0. ğŸšª Salir
```

### Motores de IA Disponibles

#### 1. OpenAI GPT-4o (Nube)

**Ventajas:**
- âœ… MÃ¡xima calidad de anÃ¡lisis
- âœ… RÃ¡pido (~2 minutos)
- âœ… ContinuaciÃ³n automÃ¡tica para respuestas largas
- âœ… HTML bien estructurado

**Desventajas:**
- âŒ Requiere API key (de pago)
- âŒ Necesita conexiÃ³n a internet
- âŒ Coste por uso ($0.10-0.30 por anÃ¡lisis)

**ConfiguraciÃ³n:**
```bash
# En .env
OPENAI_API_KEY=sk-tu-clave-aqui
OPENAI_MODEL=gpt-4o
OPENAI_MAX_TOKENS=16384
```

#### 2. Ollama GPT-OSS (Local)

**Ventajas:**
- âœ… Completamente gratuito
- âœ… Funciona sin internet
- âœ… Muy rÃ¡pido (~1 minuto)
- âœ… Privacidad total

**Desventajas:**
- âŒ Requiere 13GB de RAM para el modelo
- âŒ Calidad ligeramente inferior a GPT-4o
- âŒ Necesita instalaciÃ³n de Ollama

**InstalaciÃ³n:**
```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Descargar modelo
ollama pull gpt-oss
```

#### 3. Ollama DeepSeek-R1 (Local)

**Ventajas:**
- âœ… Completamente gratuito
- âœ… Muy eficiente (5GB modelo)
- âœ… Excelente calidad de anÃ¡lisis
- âœ… Respuestas muy detalladas

**Desventajas:**
- âŒ Ligeramente mÃ¡s lento (~2 minutos)
- âŒ Formato de salida ocasionalmente irregular

**InstalaciÃ³n:**
```bash
ollama pull deepseek-r1
```

### Ejecutar Motores Individuales

```bash
# Solo OpenAI
python generar_docs_openai.py

# Solo Ollama GPT-OSS
python generar_docs_ollama.py

# Solo DeepSeek-R1
python generar_docs_deepseek.py
```

## ğŸ“Š Resultados Generados

### Estructura de Salida

```
# Archivos index principales
index-openai.html      # Portal OpenAI
index-ollama.html      # Portal Ollama GPT-OSS
index-deepseek.html    # Portal DeepSeek-R1

# DocumentaciÃ³n web por motor
www/
â”œâ”€â”€ openai/           # HTML OpenAI
â”‚   â”œâ”€â”€ fase-F1.html
â”‚   â”œâ”€â”€ fase-F2.html
â”‚   â””â”€â”€ fase-F3.html
â”œâ”€â”€ ollama/           # HTML Ollama GPT-OSS
â”‚   â”œâ”€â”€ fase-F1.html
â”‚   â””â”€â”€ fase-F2.html
â””â”€â”€ deepseek/         # HTML DeepSeek-R1
    â”œâ”€â”€ fase-F1.html
    â””â”€â”€ fase-F2.html

# AnÃ¡lisis completos en markdown
procesados/
â”œâ”€â”€ documentacion_openai_20251113.md
â”œâ”€â”€ documentacion_ollama_20251113.md
â””â”€â”€ documentacion_deepseek_20251113.md
```

### Contenido de la DocumentaciÃ³n

Cada motor genera:

**ğŸ“„ AnÃ¡lisis de texto:**
- IdentificaciÃ³n de fases y vÃ­deos
- ResÃºmenes por vÃ­deo (corto y extendido)
- Ideas clave y puntos importantes
- Errores tÃ­picos y malos usos
- SÃ­ntesis global por fase

**ğŸŒ Web interactiva:**
- PÃ¡gina index con navegaciÃ³n
- PÃ¡ginas individuales por fase
- Cuestionarios tipo test
- NavegaciÃ³n entre pÃ¡ginas
- DiseÃ±o responsive

**ğŸ“‹ CaracterÃ­sticas especÃ­ficas:**
- **Cuestionarios**: 5-10 preguntas por fase
- **Enlaces**: NavegaciÃ³n completa entre pÃ¡ginas
- **Estilos**: CSS integrado, diseÃ±o profesional
- **JavaScript**: CorrecciÃ³n automÃ¡tica de cuestionarios

## ğŸ”§ Utilidades Adicionales

### Reparar Enlaces

Si los enlaces entre pÃ¡ginas no funcionan:

```bash
python reparar_enlaces.py
```

**Resultado:**
```
ğŸ”§ REPARADOR DE ENLACES HTML
==================================================

ğŸ” Verificando motor: OPENAI
ğŸ”§ Reparando enlaces en index-openai.html
   âœ… Enlaces reparados en index-openai.html

âœ… Proceso completado: 11 archivos reparados
```

### Verificar ConfiguraciÃ³n

```bash
python -c "
from transcribir import verificar_configuracion
verificar_configuracion()
"
```

## ğŸ“ˆ OptimizaciÃ³n de Rendimiento

### Para TranscripciÃ³n

**RTX 5090 (Ã“ptimo):**
```python
# En transcribir.py
model = faster_whisper.WhisperModel(
    "large-v3",
    device="cuda",
    compute_type="float16",
    cpu_threads=8,
    num_workers=2
)
```

**RTX 4080/4090:**
```python
model = faster_whisper.WhisperModel(
    "large-v3",
    device="cuda", 
    compute_type="float16",
    cpu_threads=6,
    num_workers=1
)
```

**RTX 3080/3090:**
```python
model = faster_whisper.WhisperModel(
    "medium",  # Modelo mÃ¡s pequeÃ±o
    device="cuda",
    compute_type="float16",
    cpu_threads=4,
    num_workers=1
)
```

### Para IA Local

**Para Ollama con poca RAM:**
```bash
# Modelos alternativos mÃ¡s pequeÃ±os
ollama pull llama2:7b      # 3.8GB
ollama pull phi3:mini      # 2.3GB

# En .env
OLLAMA_MODEL=llama2:7b
```

## ğŸ¨ PersonalizaciÃ³n

### Modificar Prompts

Los prompts estÃ¡n en `transcribir.py`:

```python
def crear_prompt_maestro_original(transcripciones_consolidadas):
    """Modificar este prompt para personalizar el anÃ¡lisis"""
    prompt = f"""
    Tu prompt personalizado aquÃ­...
    
    Transcripciones: {transcripciones_consolidadas}
    """
    return prompt
```

### Personalizar Estilos Web

Los estilos CSS se generan automÃ¡ticamente, pero puedes modificarlos editando la funciÃ³n `actualizar_enlaces_html()`.

### AÃ±adir Nuevos Motores

1. Crear archivo `generar_docs_nuevo_motor.py`
2. Implementar funciÃ³n `generar_documentacion_con_nuevo_motor()`
3. AÃ±adir opciÃ³n al menÃº en `generar_docs.py`

## ğŸ” Monitoreo y Debugging

### Logs de TranscripciÃ³n

```bash
# Ver progreso en tiempo real
tail -f procesados/registro_transcripciones.txt
```

### EstadÃ­sticas de Rendimiento

```bash
# Ver mÃ©tricas detalladas
cat procesados/estadisticas_transcripcion.json | jq '.'
```

### Debug de IA

Cada motor guarda informaciÃ³n de debugging:

```bash
# Hash Ãºnico para verificar creatividad
grep "Hash Ãºnico" procesados/documentacion_*.md

# Tiempo de ejecuciÃ³n
grep "Tiempo de generaciÃ³n" procesados/documentacion_*.md
```

## ğŸš¨ SoluciÃ³n de Problemas Comunes

### Error: No se detecta GPU

```bash
python -c "import torch; print(torch.cuda.is_available())"
```

**Si imprime `False`:**
1. Verificar instalaciÃ³n CUDA: `nvcc --version`
2. Reinstalar PyTorch con CUDA
3. Reiniciar sistema

### Error: Ollama no responde

```bash
# Reiniciar servicio
pkill ollama
ollama serve
```

### Error: OpenAI rate limit

Esperar 1 minuto o verificar lÃ­mites de API en el dashboard de OpenAI.

### Calidad pobre de transcripciÃ³n

1. Verificar calidad de audio del vÃ­deo
2. Usar modelo mÃ¡s grande (`large-v3` en lugar de `medium`)
3. Cambiar `compute_type` a `float32`

---

**ğŸ“Œ Siguiente paso**: [Referencia de la API](API_REFERENCE.md)